{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Example usage of the `torchTextClassifiers` library\n",
    "\n",
    "*Warning*\n",
    "\n",
    "*`torchTextClassifiers` library is still under active development. Have a\n",
    "regular look to <https://github.com/inseefrlab/torchTextClassifiers> for\n",
    "latest information.*\n",
    "\n",
    "To download the latest (development) version of the library, you can use\n",
    "```bash\n",
    "uv add git+https://github.com/InseeFrLab/torchTextClassifiers\n",
    "```\n",
    "or, if you prefer using `pip`:\n",
    "```bash\n",
    "pip install git git+https://github.com/InseeFrLab/torchTextClassifiers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from notebooks.utils import categorize_surface, clean_and_tokenize_df\n",
    "from torchTextClassifiers import ModelConfig, TrainingConfig, torchTextClassifiers\n",
    "from torchTextClassifiers.dataset import TextClassificationDataset\n",
    "from torchTextClassifiers.model import TextClassificationModel, TextClassificationModule\n",
    "from torchTextClassifiers.model.components import (\n",
    "    AttentionConfig,\n",
    "    CategoricalVariableNet,\n",
    "    ClassificationHead,\n",
    "    TextEmbedder,\n",
    "    TextEmbedderConfig,\n",
    ")\n",
    "from torchTextClassifiers.tokenizers import HuggingFaceTokenizer, WordPieceTokenizer\n",
    "from torchTextClassifiers.utilities.plot_explainability import (\n",
    "    map_attributions_to_char,\n",
    "    map_attributions_to_word,\n",
    "    plot_attributions_at_char,\n",
    "    plot_attributions_at_word,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "In that guide, we propose to illustrate main package functionalities\n",
    "using that `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-ape/data/08112022_27102024/naf2008/split/df_train.parquet\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "It contains an extract from the French business register (SIRENE). The `apet_finale` column contains the activity code - out target - in the French version of the [NACE nomenclature](https://ec.europa.eu/eurostat/web/products-manuals-and-guidelines/-/ks-ra-07-015). The text to classify is the `libelle` column, which contains a short description of the activity.\n",
    "\n",
    "Other columns are additional, **categorical** features that will be also used as inputs to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [ \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Right now, the model requires the label (variable y) to be a numerical\n",
    "variable. If the label variable is a text variable, we recommend using\n",
    "Scikit Learn’s\n",
    "[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "to convert into a numeric variable. Using that function will give user\n",
    "the possibility to get back labels from the encoder after running\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "The function `clean_and_tokenize_df` requires special `DataFrame`\n",
    "formatting:\n",
    "\n",
    "-   First column contains the processed text (str)\n",
    "-   Next ones contain the **“encoded”** categorical (discrete) variables in\n",
    "    **int format**, as required by torchTextClassifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle\")\n",
    "X = df[[\"libelle\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "We now split the data into train, val and test sets, as done classically in machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Let's dive into the different components of a text classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = X_train[:, 0].tolist()  # extract the text column as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "You can directly load a pretrained tokenizer from Hugging Face. But you won't have control over its vocabulary size or other parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HuggingFaceTokenizer.load_from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "print(\"This tokenizer outputs tensors of size \", tokenizer.tokenize(text[0]).input_ids.shape)\n",
    "print(\"The tokens are here \", tokenizer.tokenizer.convert_ids_to_tokens(tokenizer.tokenize(text[0]).input_ids.squeeze(0)))\n",
    "print(\"The total number of tokens is \", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Or you can train your own tokenizer from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPieceTokenizer(vocab_size=5000, output_dim=125)\n",
    "tokenizer.train(text)\n",
    "print(\"This tokenizer outputs tensors of size \", tokenizer.tokenize(text[0]).input_ids.shape)\n",
    "print(\"The tokens are here \", tokenizer.tokenizer.convert_ids_to_tokens(tokenizer.tokenize(text[0]).input_ids.squeeze(0)))\n",
    "print(\"The total number of tokens is \", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## The PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "To train a text classification model using PyTorch, you need to create a Dataset object that will handle the data loading and preprocessing. The `TextClassificationDataset` class from the `torchTextClassifiers` library can be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextClassificationDataset(\n",
    "    texts=X_train[:, 0].tolist(),\n",
    "    categorical_variables=X_train[:, 1:].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    labels=y_train,\n",
    ")\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "And then, you can create a ``DataLoader`` to iterate over the dataset during training. The DataLoader, via its `collate_fn` function, handles nicely the raw text and outputs tokenized, padded PyTorch tensors for immediate model ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = train_dataset.create_dataloader(\n",
    "            batch_size=256,\n",
    "            num_workers=12,\n",
    "            shuffle=False,\n",
    "        )\n",
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "print(\"Input IDs shape: \", batch[\"input_ids\"].shape)  # (batch_size, tokenizer.output_dim (=seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## The PyTorch Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We provide three main components for the model architecture: \n",
    "\n",
    "- The `TextEmbedder` class, which handles the embedding of the token ids input into dense vectors\n",
    "- The `CategoricalVariableNet` class, which handles the embedding of the categorical variables\n",
    "- The `ClassificationHead`, that outputs the prediction vector\n",
    "\n",
    "Eventually, the `TextClassificationModel` class combines all these components into a single model that can be trained end-to-end.\n",
    "\n",
    "All of these four objects inherit from the `torch.nn.Module` class, so you can use them as you would do with any PyTorch model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### The TextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TextEmbedder parameters\n",
    "\n",
    "# size of the vocabulary - do not change, this must match the tokenizer used. It is the number of rows in the embedding matrix\n",
    "vocab_size = tokenizer.vocab_size  \n",
    "padding_idx = tokenizer.padding_idx\n",
    "embedding_dim = 96\n",
    "\n",
    "### Attention parameters - Optional ! If you want to add self-attention layer\n",
    "n_layers = 1\n",
    "n_head = 4\n",
    "n_kv_head = n_head\n",
    "sequence_len = tokenizer.output_dim\n",
    "\n",
    "# wrap them into AttentionConfig\n",
    "attention_config = AttentionConfig(\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    n_kv_head=n_kv_head,\n",
    "    sequence_len=sequence_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The first component is the embedding layer. It transforms input tokens (that comes from the tokenizer) into dense vectors of dimension `embedding_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap all TextEmbedder parameters into TextEmbedderConfig\n",
    "text_embedder_config = TextEmbedderConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=padding_idx,\n",
    "    attention_config=attention_config,\n",
    ")\n",
    "\n",
    "# initialize the TextEmbedder\n",
    "text_embedder = TextEmbedder(\n",
    "    text_embedder_config=text_embedder_config,\n",
    ")\n",
    "text_embedder.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the TextEmbedder: it takes as input a tensor of token ids and outputs a tensor of embeddings\n",
    "text_embedder_output = text_embedder(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "\n",
    "print(\"TextEmbedder input: \", text_embedder_input.input_ids)\n",
    "print(\"TextEmbedder output shape: \", text_embedder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### The CategoricalVariableNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The second component is the categorical variable layer. It transforms input ids from the encoded categorical variables into dense vectors of dimension `categorical_embedding_dims`. Depending on the type of `categorical_embedding_dims`, the output dimension will vary.\n",
    "\n",
    "This component is of course optional: you will not use it if you don't have categorical variables in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vocab_sizes = (X[:, 1:].max(axis=0) + 1).tolist()\n",
    "\n",
    "## categorical_embedding_dims can be a list of ints (one embedding dimension per categorical variable) - then they are concatenated\n",
    "# the final average will be concatenated with text embedding (in the full model)\n",
    "categorical_embedding_dims = [15, 10, 10, 10, 10]\n",
    "categorical_var_net = CategoricalVariableNet(\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    ")\n",
    "cat_var_net_output = categorical_var_net(batch[\"categorical_vars\"])\n",
    "print(cat_var_net_output.shape)\n",
    "print(\"How will the categorical embedding be merged with the text one ? \", categorical_var_net.forward_type)\n",
    "\n",
    "## it can also be None - the output dim will be text_embedding_dim that you should specify\n",
    "# the final average will be ADDED TO text embedding (in the full model)\n",
    "categorical_embedding_dims = None\n",
    "categorical_var_net = CategoricalVariableNet(\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    text_embedding_dim=embedding_dim,  # not putting this will raise an error\n",
    ")\n",
    "cat_var_net_output = categorical_var_net(batch[\"categorical_vars\"])\n",
    "print(cat_var_net_output.shape)\n",
    "print(\"How will the categorical embedding be merged with the text one ? \", categorical_var_net.forward_type)\n",
    "\n",
    "\n",
    "## and finally, it can be an int (the same embedding dimension for all categorical variables and then averaging)\n",
    "# the final average will be concatenated with text embedding (in the full model)\n",
    "categorical_embedding_dims = 25\n",
    "categorical_var_net = CategoricalVariableNet(\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    ")\n",
    "cat_var_net_output = categorical_var_net(batch[\"categorical_vars\"])\n",
    "print(cat_var_net_output.shape)\n",
    "print(\"How will the categorical embedding be merged with the text one ? \", categorical_var_net.forward_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### The ClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = int(y.max() + 1)\n",
    "\n",
    "# as discussed above, the input dim of the classification head depends on how categorical variables are handled\n",
    "if type(categorical_embedding_dims) is int or type(categorical_embedding_dims) is list:\n",
    "    expected_input_dim = embedding_dim + categorical_var_net.output_dim\n",
    "else:\n",
    "    expected_input_dim = embedding_dim\n",
    "\n",
    "classification_head = ClassificationHead(\n",
    "    input_dim=expected_input_dim,\n",
    "    num_classes=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_combined = torch.cat((text_embedder_output, cat_var_net_output), dim=1)\n",
    "logits = classification_head(x_combined)\n",
    "print(\"logits shape: \", logits.shape)  # (batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### The TextClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "Finally, the `TextClassificationModel` class combines all these components into a single model that can be trained end-to-end.\n",
    "It checks if everything connects well together (input/output dimensions) and handles the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    text_embedder=text_embedder,\n",
    "    categorical_variable_net=categorical_var_net,\n",
    "    classification_head=classification_head,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the same input as TextEmbedder + CategoricalVarNet -> same output as ClassificationHead (logits)\n",
    "model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], categorical_vars=batch[\"categorical_vars\"]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### The TextClassificationModule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "We provide a PyTorch Lightning wrapper, for easy training and checkpointing. We refer to [Lightning's doc](https://lightning.ai/docs/pytorch/stable/) for more details on how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "module = TextClassificationModule(\n",
    "    model=model,\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={\"lr\": 1e-3},\n",
    "    scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# `torchTextClassifiers`: a wrapper to handle them all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two main config objects, that mirror the parameters used above - and you're good to go !\n",
    "\n",
    "model_config = ModelConfig(\n",
    "    embedding_dim=embedding_dim,\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    num_classes=num_classes,\n",
    "    attention_config=attention_config,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    lr=1e-3,\n",
    "    batch_size=256,\n",
    "    num_epochs=10,\n",
    ")\n",
    "\n",
    "ttc = torchTextClassifiers(\n",
    "    tokenizer=tokenizer,\n",
    "    model_config=model_config,\n",
    ")\n",
    "\n",
    "## Given those parameters, the TextClassificationModel is created internally, with the right connections between components\n",
    "ttc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "`torchTextClassifiers` has a `.train()` method that handles the whole training process for you:\n",
    "\n",
    "- Init of dataset and dataloaders\n",
    "- Init of Lightning module\n",
    "- Training with early stopping and model checkpointing\n",
    "- Using Lightning's Trainer under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_test,\n",
    "    y_val=y_test,\n",
    "    training_config=training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "## Prediction and explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc.pytorch_model.eval().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 5\n",
    "yyy = ttc.predict(X_test[:10], top_k=top_k, explain=True)\n",
    "\n",
    "text_idx = 0\n",
    "text = X_test[text_idx, 0]\n",
    "offsets = yyy[\"offset_mapping\"][text_idx]  # seq_len, 2\n",
    "attributions = yyy[\"attributions\"][text_idx]  # top_k, seq_len\n",
    "word_ids = yyy[\"word_ids\"][text_idx]  # seq_len\n",
    "predictions = yyy[\"prediction\"][text_idx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = map_attributions_to_word(attributions, word_ids)\n",
    "char_attributions = map_attributions_to_char(attributions, offsets, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.inverse_transform(np.array([predictions]).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = plot_attributions_at_char(\n",
    "    text=text,\n",
    "    attributions_per_char=char_attributions,\n",
    "    titles = list(map(lambda x: f\"Attributions for code {x}\", encoder.inverse_transform(np.array([predictions]).reshape(-1)).tolist())),\n",
    ")\n",
    "figshow(all_plots[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = plot_attributions_at_word(\n",
    "    text=text,\n",
    "    attributions_per_word=word_attributions,\n",
    "    titles = list(map(lambda x: f\"Attributions for code {x}\", encoder.inverse_transform(np.array([predictions]).reshape(-1)).tolist())),\n",
    ")\n",
    "figshow(all_plots[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
