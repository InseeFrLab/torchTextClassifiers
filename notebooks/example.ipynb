{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Example usage of the `torchTextClassifiers` library\n",
    "\n",
    "*Warning*\n",
    "\n",
    "*`torchTextClassifiers` library is still under active development. Have a\n",
    "regular look to <https://github.com/inseefrlab/torchTextClassifiers> for\n",
    "latest information.*\n",
    "\n",
    "To install package, you can run the following snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable version\n",
    "%uv pip install --system .. \n",
    "%uv pip install --system captum unidecode nltk scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchTextClassifiers import ModelConfig, TrainingConfig, torchTextClassifiers\n",
    "from torchTextClassifiers.dataset import TextClassificationDataset\n",
    "from torchTextClassifiers.model import TextClassificationModel, TextClassificationModule\n",
    "from torchTextClassifiers.model.components import (\n",
    "    AttentionConfig,\n",
    "    CategoricalVariableNet,\n",
    "    ClassificationHead,\n",
    "    TextEmbedder,\n",
    "    TextEmbedderConfig,\n",
    ")\n",
    "from torchTextClassifiers.tokenizers import HuggingFaceTokenizer, WordPieceTokenizer\n",
    "from torchTextClassifiers.utilities.plot_explainability import (\n",
    "    map_attributions_to_char,\n",
    "    map_attributions_to_word,\n",
    "    plot_attributions_at_char,\n",
    "    plot_attributions_at_word,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Load and preprocess data\n",
    "\n",
    "In that guide, we propose to illustrate main package functionalities\n",
    "using that `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"https://minio.lab.sspcloud.fr/projet-ape/data/08112022_27102024/naf2008/split/df_train.parquet\")\n",
    "df = df.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Our goal will be to build multilabel classification for the `code`\n",
    "variable using `libelle` as feature.\n",
    "\n",
    "## Enriching our test dataset\n",
    "\n",
    "Unlike `Fasttext`, this package offers the possibility of having several\n",
    "feature columns of different types (string for the text column and\n",
    "additional variables in numeric form, for example). To illustrate that,\n",
    "we propose the following enrichment of the example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def categorize_surface(\n",
    "    df: pd.DataFrame, surface_feature_name: int, like_sirene_3: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Categorize the surface of the activity.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to categorize.\n",
    "        surface_feature_name (str): Name of the surface feature.\n",
    "        like_sirene_3 (bool): If True, categorize like Sirene 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with a new column \"surf_cat\".\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].replace(\"nan\", np.nan)\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(float)\n",
    "    # Check surface feature exists\n",
    "    if surface_feature_name not in df.columns:\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} not found in DataFrame.\")\n",
    "    # Check surface feature is a float variable\n",
    "    if not (pd.api.types.is_float_dtype(df_copy[surface_feature_name])):\n",
    "        raise ValueError(f\"Surface feature {surface_feature_name} must be a float variable.\")\n",
    "\n",
    "    if like_sirene_3:\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy[surface_feature_name],\n",
    "            bins=[0, 120, 400, 2500, np.inf],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "    else:\n",
    "        # Log transform the surface\n",
    "        df_copy[\"surf_log\"] = np.log(df[surface_feature_name])\n",
    "\n",
    "        # Categorize the surface\n",
    "        df_copy[\"surf_cat\"] = pd.cut(\n",
    "            df_copy.surf_log,\n",
    "            bins=[0, 3, 4, 5, 12],\n",
    "            labels=[\"1\", \"2\", \"3\", \"4\"],\n",
    "        ).astype(str)\n",
    "\n",
    "    df_copy[surface_feature_name] = df_copy[\"surf_cat\"].replace(\"nan\", \"0\")\n",
    "    df_copy[surface_feature_name] = df_copy[surface_feature_name].astype(int)\n",
    "    df_copy = df_copy.drop(columns=[\"surf_log\", \"surf_cat\"], errors=\"ignore\")\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "def clean_and_tokenize_df(\n",
    "    df,\n",
    "    categorical_features=[\"CJ\", \"NAT\", \"TYP\", \"CRT\"],\n",
    "    text_feature=\"libelle_processed\",\n",
    "    label_col=\"apet_finale\",\n",
    "):\n",
    "    df.fillna(\"nan\", inplace=True)\n",
    "    les = []\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        les.append(le)\n",
    "\n",
    "    df = categorize_surface(df, \"SRF\", like_sirene_3=True)\n",
    "    df = df[[text_feature,  \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\", label_col]]\n",
    "\n",
    "    return df, les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [ \"CJ\", \"NAT\", \"TYP\", \"SRF\", \"CRT\"]\n",
    "text_feature = \"libelle\"\n",
    "y = \"apet_finale\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Right now, the model requires the label (variable y) to be a numerical\n",
    "variable. If the label variable is a text variable, we recommend using\n",
    "Scikit Learn’s\n",
    "[LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n",
    "to convert into a numeric variable. Using that function will give user\n",
    "the possibility to get back labels from the encoder after running\n",
    "predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df[\"apet_finale\"] = encoder.fit_transform(df[\"apet_finale\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The function `clean_and_tokenize_df` requires special `DataFrame`\n",
    "formatting:\n",
    "\n",
    "-   First column contains the processed text (str)\n",
    "-   Next ones contain the “encoded” categorical (discrete) variables in\n",
    "    int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _ = clean_and_tokenize_df(df, text_feature=\"libelle\")\n",
    "X = df[[\"libelle\", \"CJ\", \"NAT\", \"TYP\", \"CRT\", \"SRF\"]].values\n",
    "y = df[\"apet_finale\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Splitting in train-test sets\n",
    "\n",
    "As usual in a learning approach, you need to break down your data into\n",
    "learning and test/validation samples to obtain robust performance\n",
    "statistics.\n",
    "This work is the responsibility of the package’s users. Please make sure that np.max(y_train) == len(np.unique(y_train))-1 (i.e. your labels are well encoded, in a consecutive manner, starting from 0), and that all the possible labels appear at least once in the training set.\n",
    "\n",
    "We provide the function stratified_train_test_split to match these requirements here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = X_train[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = HuggingFaceTokenizer.load_from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "tokenizer.tokenize(text[0]).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = WordPieceTokenizer(5000, output_dim=125)\n",
    "tokenizer.train(text)\n",
    "tokenizer.tokenize(text[:256]).input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Consider each component indepedently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "padding_idx = tokenizer.padding_idx\n",
    "\n",
    "embedding_dim = 96\n",
    "n_layers = 1\n",
    "n_head = 4\n",
    "n_kv_head = n_head\n",
    "sequence_len = tokenizer.output_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_config = AttentionConfig(\n",
    "    n_layers=n_layers,\n",
    "    n_head=n_head,\n",
    "    n_kv_head=n_kv_head,\n",
    "    sequence_len=sequence_len,\n",
    ")\n",
    "\n",
    "text_embedder_config = TextEmbedderConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    padding_idx=padding_idx,\n",
    "    attention_config=attention_config,\n",
    ")\n",
    "\n",
    "\n",
    "text_embedder = TextEmbedder(\n",
    "    text_embedder_config=text_embedder_config,\n",
    ")\n",
    "text_embedder.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 1:].max(axis=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vocab_sizes = (X[:, 1:].max(axis=0) + 1).tolist()\n",
    "categorical_embedding_dims = 25\n",
    "\n",
    "categorical_var_net = CategoricalVariableNet(\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = int(y.max() + 1)\n",
    "expected_input_dim = embedding_dim + categorical_var_net.output_dim\n",
    "classification_head = ClassificationHead(\n",
    "    input_dim=expected_input_dim,\n",
    "    num_classes=num_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassificationModel(\n",
    "    text_embedder=text_embedder,\n",
    "    categorical_variable_net=categorical_var_net,\n",
    "    classification_head=classification_head,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "module = TextClassificationModule(\n",
    "    model=model,\n",
    "    loss=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={\"lr\": 1e-3},\n",
    "    scheduler=None,\n",
    "    scheduler_params=None,\n",
    "    scheduler_interval=\"epoch\",\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Using the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig(\n",
    "    embedding_dim=embedding_dim,\n",
    "    categorical_vocabulary_sizes=categorical_vocab_sizes,\n",
    "    categorical_embedding_dims=categorical_embedding_dims,\n",
    "    num_classes=num_classes,\n",
    "    attention_config=attention_config,\n",
    ")\n",
    "\n",
    "training_config = TrainingConfig(\n",
    "    lr=1e-3,\n",
    "    batch_size=256,\n",
    "    num_epochs=10,\n",
    ")\n",
    "\n",
    "ttc = torchTextClassifiers(\n",
    "    tokenizer=tokenizer,\n",
    "    model_config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(X_train[:256, 0].tolist()).input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttc.train(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_test,\n",
    "    y_val=y_test,\n",
    "    training_config=training_config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
